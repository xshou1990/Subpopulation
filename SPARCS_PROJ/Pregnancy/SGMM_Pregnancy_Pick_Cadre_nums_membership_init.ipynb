{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../sgmm')\n",
    "sys.path.append('../metrics')\n",
    "sys.path.append('../loaders')\n",
    "sys.path.append('../Misc')\n",
    "sys.path.append('../visual')\n",
    "sys.path.append('../otherModels')\n",
    "sys.path.append('../LogOdds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervisedGmm import SupervisedGMM\n",
    "from metricsFunctions import calc_metrics, CalculateSoftLogReg, optimalTau,metrics_cluster,sgmmResults\n",
    "# from loaders2 import loader\n",
    "# from mlModels import logisticRegressionCv2, neural_nets, randomforests,\\\n",
    "# kmeansLogRegr, xboost, gradboost,kmeansBNB\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from utility import entropy,asymm_entropy\n",
    "# from ftest_logodds import ftest_uncorr\n",
    "# from ftest_logodds import restest\n",
    "# #from clustmap import plotclustmap\n",
    "# from clustmap_newborn_kmeans_noopt import plotclustmap\n",
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100679 244\n"
     ]
    }
   ],
   "source": [
    "sparcs = pd.read_csv(\"~/data/CDPHP/xiao/SPARCS_Subsets/Obsolete/sparcs25%Preg_DeHos_Outflow_Region.csv\") \n",
    "\n",
    "#sparcs = sparcs.sample(frac=0.02, random_state = 1512)\n",
    "\n",
    "d_preg_tr, d_preg_te = train_test_split(sparcs, test_size=0.2, random_state = 1512)\n",
    "\n",
    "print(d_preg_tr.shape[0], d_preg_tr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric matrix columns\n",
    "columns = ['cluster', 'size', 'high_cost%','low_cost%', \n",
    "                       'TP', 'TN', 'FP', 'FN', \n",
    "                       'FPR', 'specificity', 'sensitivity', 'precision',\n",
    "                       'accuracy', 'balanced accuracy', 'f1', 'auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature list\n",
    "features = list(sparcs.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the data\n",
    "Xtrain, Xtest = d_preg_tr.iloc[:,0:-1].values, d_preg_te.iloc[:,0:-1].values\n",
    "ytrain, ytest = d_preg_tr.iloc[:,-1].values.astype(int), d_preg_te.iloc[:,-1].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80543, 244) (20136, 244)\n",
      "\n",
      " TRAINING WITH RANDOM SEED 19977\n",
      "\n",
      "\n",
      "\n",
      "TRAINING WITH 2 CLUSTERS\n",
      "GMM iteration: 0, error: 0.4207138630983965\n",
      "GMM iteration: 1, error: 0.17119334388915566\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION ON CLUSTERS\n",
    "#RUN VALIDATION ON NUMBER OF CLUSTERS BASED ON AUC & F1\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "train, val = train_test_split(d_preg_tr, test_size = 0.2, random_state = 1512)\n",
    "y_train, y_val = train.iloc[:,-1], val.iloc[:,-1]\n",
    "\n",
    "print( train.shape, val.shape )\n",
    "\n",
    "#SET SGMM PARAMETERS\n",
    "\n",
    "max_iter = 30\n",
    "max_iter2 = 30\n",
    "\n",
    "train_np = train.iloc[:,0:-1].values\n",
    "val_np = val.iloc[:,0:-1].values\n",
    "\n",
    "y_train_np = y_train.values\n",
    "y_val_np = y_val.values\n",
    "\n",
    "test_re = []\n",
    "train_re = []\n",
    "seed_list = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for j in range(0,10): \n",
    "\n",
    "    seed_num = random.randint(0,100000)\n",
    "    seed_list.append(seed_num)\n",
    "    print(\"\\n TRAINING WITH RANDOM SEED {}\". format(seed_num))\n",
    "#test clusters 2-10\n",
    "    for i in range(2,11):\n",
    "        print(\"\\n\\n\\nTRAINING WITH {} CLUSTERS\".format(i))\n",
    "        np.random.seed( seed = seed_num )\n",
    "        model = SupervisedGMM(max_iter = max_iter, max_iter2 = max_iter2, n_clusters = i)\n",
    "        model = model.fit(Xtrain = train_np, ytrain = y_train_np)\n",
    "        probTrain = model.predict_proba( train_np )\n",
    "        probTest = model.predict_proba( val_np )\n",
    "        results = sgmmResults( model, probTest.copy(), probTrain.copy(), y_val_np.copy(), y_train_np.copy(), tau = None,\n",
    "            mode = 0)\n",
    "        testmetrics = results['testMet']\n",
    "        trainMetrics = results[ 'trainMet']\n",
    "        test_re.append( testmetrics )\n",
    "        train_re.append( trainMetrics )\n",
    "    \n",
    "end = time.time() - start\n",
    "print(\"time elapsed: {}\".format( end ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICK THE CLUSTERS\n",
    "\n",
    "pdi = pd.DataFrame(np.zeros(shape = [len(test_re), test_re[0].shape[1]]), columns = columns)\n",
    "pdi_tr = pd.DataFrame(np.zeros(shape = [len(train_re), train_re[0].shape[1]]), columns = columns)\n",
    "\n",
    "for i, panda in enumerate(test_re):\n",
    "     pdi.iloc[i,:] = panda.iloc[0,:]\n",
    "        \n",
    "for i, panda in enumerate(train_re):\n",
    "     pdi_tr.iloc[i,:] = panda.iloc[0,:]\n",
    "\n",
    "pdi['seed_number'] = pd.DataFrame(np.repeat(np.array(seed_list),9) )\n",
    "pdi['cluster']= pd.DataFrame(np.tile(np.arange(2,11),10))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the number of clusters and seed_number for the largest auc \n",
      "cluster           8.000000\n",
      "auc               0.820617\n",
      "seed_number    3957.000000\n",
      "Name: 69, dtype: float64\n",
      "Here are the number of clusters and seed_number for the smallest auc \n",
      "cluster            8.000000\n",
      "auc                0.817626\n",
      "seed_number    98215.000000\n",
      "Name: 60, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Here are the number of clusters and seed_number for the largest auc ')\n",
    "print(pdi.sort_values(by = 'auc',ascending=False).iloc [0,:][['cluster','auc','seed_number']])\n",
    "print('Here are the number of clusters and seed_number for the smallest auc ')\n",
    "print(pdi.sort_values(by = 'auc',ascending=True).iloc [0,:][['cluster','auc','seed_number']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # pdi.to_csv('SGMM_Pregnancy_Pick_Cadre_nums_membership_init.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (health3)",
   "language": "python",
   "name": "health3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
